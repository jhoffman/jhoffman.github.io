{
  "id": "2020NeurIPSAux",
  "authors": "Baifeng Shi, Judy Hoffman, Kate Saenko, Trevor Darrell, Huijuan Xu",
  "title": "Auxiliary Task Reweighting for Minimum-data Learning",
  "venue": "neurips",
  "year": "2020",
  "pdf": "",
  "slides": "",
  "code": "https://github.com/bfshi/ARML_Auxiliary_Task_Reweighting",
  "bibtex_type": "inproceedings",
  "fig": "figs/2020NeurIPSAux.png",
  "project": "https://sites.google.com/view/auxiliary-task-reweighting",
  "video": "https://youtu.be/B2i6z6HefGw",
  "is_new": false,
  "special": "",
  "abstract": "Supervised learning requires a large amount of training data, limiting its appli- cation where labeled data is scarce. To compensate for data scarcity, one pos- sible method is to utilize auxiliary tasks to provide additional supervision for the main task. Assigning and optimizing the importance weights for different auxiliary tasks remains an crucial and largely understudied research question. In this work, we propose a method to automatically reweight auxiliary tasks in order to reduce the data requirement on the main task. Specifically, we formu- late the weighted likelihood function of auxiliary tasks as a surrogate prior for the main task. By adjusting the auxiliary task weights to minimize the diver- gence between the surrogate prior and the true prior of the main task, we obtain a more accurate prior estimation, achieving the goal of minimizing the required amount of training data for the main task and avoiding a costly grid search. In multiple experimental settings (e.g. semi-supervised learning, multi-label classifi- cation), we demonstrate that our algorithm can effectively utilize limited labeled data of the main task with the benefit of auxiliary tasks compared with previous task reweighting methods. We also show that under extreme cases with only a few extra examples (e.g. few-shot domain adaptation), our algorithm results in significant improvement over the baseline.",
  "month": 12
}